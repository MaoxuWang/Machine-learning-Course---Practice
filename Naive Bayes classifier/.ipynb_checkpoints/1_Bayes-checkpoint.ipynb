{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **数据挖掘课程设计一：Titanic船员生存预测**\n",
    "> written by ***Maoxu Wang*** from SCU, majored in **Computational Biology**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **数据格式**：\n",
    "\n",
    "|Attribute |Type |Range        |\n",
    "|----------|:---:|------------:|\n",
    "|Class     |real |[-1.87,0.965]|     \n",
    "|Age       |real |[-0.228,4.38]|    \n",
    "|Sex       |real |[-1.92,0.521]|    \n",
    "|Survived  |int  |{-1.0,1.0}   |          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### **输入**：\n",
    "\n",
    "* #### inputs Class, Age, Sex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **输出**：\n",
    "\n",
    "* #### outputs Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **目标**：\n",
    "1.\t对各个属性的值进行离散化，离散化成两个区间（即把各个属性的取值变成布尔类型）。要求以信息增益作为标准，对每个属性选择信息增益最大的区间划分点（也叫做阈值点）；\n",
    "2.\t对给定的数据集随机划分，70%作为训练数据，30%作为测试数据；\n",
    "3.\t实现Naïve Bayes算法，给出测试数据集中每个测试样例的预测类标，同时输出每个测试样例属于每个类别的后验概率值。\n",
    "4.\t统计算法在测试集上类别预测的准确率（预测类别正确的测试样例的个数/测试样例的个数）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、 **数据清洗** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 加载相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 根据信息增益分隔区间，离散化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1** 连续属性离散化技术：采用二分法处理\n",
    "> 给定样本集D和连续属性a，假定a在D上出现了n个不同的取值，将这些值从小到大进行排序，记为${a^1.a^2,\\dots ,a^n}$,基于划分点t将D分为子集$D^-_t$和$D^+_t$，其中$D^-_t$包含属性a上取值不大于t的样本，而$D^+_t$则包含在属性a上取值大于t的样本，考察包含n-1个元素的候选划分点集合\n",
    "$$\n",
    "T_a=\\left[\n",
    "        \\frac{a^i\\,+\\,a^{i+1}}{2}\\,|\\,1\\leq i \\leq {n-1} \n",
    "    \\right]\n",
    "$$\n",
    "即把区间的中位点$\\frac{a^i\\,+\\,a^{i+1}}{2}$作为候选划分点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1.2** 信息熵是度量样本集合纯度最常用的一种指标，假定当前样本集合D中第k类样本所占比例为$p_k (k = 1,2,...|\\Upsilon|)$,则D的信息熵定义为\n",
    "\n",
    "$$\n",
    "Ent(D) = -\\sum_{k=1}^{|\\Upsilon|} p_klog_2p_k .\n",
    "$$ \n",
    "> #### 假定离散属性值a有V个可能的取值${a^1,a^2,...,a^V}$,若使用a对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为$a^v$的样本，记位$D^v$. 可根据上式计算$D_v$的信息熵，考虑道不同的分支节点所包含的样本数不同，给分支结点赋予权重$\\frac {|D^v|}{|D|}$,即样本数越多的分支结点影响越大，于是可计算出用属性a对样本集D进行划分所获得的信息增益：\n",
    "\n",
    "$$\n",
    "Gain(D, a) = Ent(D)-\\sum_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)\n",
    "$$\n",
    "> #### 一般而言，信息增益越大，则意味者使用属性a来进化划分所获得的\"纯度提升\"越大，即选择属性$a_* = arg\\,max\\,Gain(D,a)\\,,a\\in A$来进行划分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_entropy(dataset):\n",
    "    \"\"\"\n",
    "    计算给定数据集下，按照两类分的情况的信息熵\n",
    "    :param dataset:数据集\n",
    "    :return:信息熵\n",
    "    \"\"\"\n",
    "    if(len(dataset) == 0):\n",
    "        return 0\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for row in dataset:\n",
    "        if(row[-1] == 1):\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    Pr_P = positive / len(dataset)\n",
    "    Pr_N = negative / len(dataset)\n",
    "    Ent_ = -(Pr_P*math.log2(Pr_P) + Pr_N*math.log2(Pr_N))\n",
    "    return Ent_\n",
    "    \n",
    "def information_gain(dataset, index_attribute, value):\n",
    "    \"\"\"\n",
    "    计算给定属性、按照给定值做划分的信息增益\n",
    "    :param dataset:数据集\n",
    "    :param index_attribute:属性序列\n",
    "    :param value:当前属性下的给定取值\n",
    "    :return ：信息增益\n",
    "    \"\"\"\n",
    "    Ent_D = information_entropy(dataset)\n",
    "    new_dataset_P = []\n",
    "    new_dataset_N = []\n",
    "    for row in dataset:\n",
    "        if(row[index_attribute] > value):\n",
    "            new_dataset_P.append(row)\n",
    "        else:\n",
    "            new_dataset_N.append(row)\n",
    "    new_dataset_P = np.array(new_dataset_P)\n",
    "    new_dataset_N = np.array(new_dataset_N)\n",
    "    Ent_val_P = information_entropy(new_dataset_P)        \n",
    "    Ent_val_N = information_entropy(new_dataset_N)\n",
    "    Ent_new = (len(new_dataset_P)*Ent_val_P + len(new_dataset_N)*Ent_val_N) / len(dataset)\n",
    "    return (Ent_D-Ent_new)\n",
    "    \n",
    "def middle_point(num):\n",
    "    \"\"\"\n",
    "    求一个数组的从小到大排列后的中间值\n",
    "    :param num:任意一个长度为n的数组\n",
    "    :return (n-1)长度的数族\n",
    "    \"\"\"\n",
    "    ls = []\n",
    "    num.sort()\n",
    "    for i in range(len(num)-1):\n",
    "        ls.append((num[i]+num[i+1])/2)\n",
    "    return ls\n",
    "def split_attribute(dataset):\n",
    "    \"\"\"\n",
    "    按照最大信息增益划分区间，从小到大排列连续值的中间点为候选划分点\n",
    "    ：param dataset：数据集\n",
    "    ：return:每个属性划分为两个区间的数据集\n",
    "    \"\"\"\n",
    "    for i in range(3):\n",
    "        matrix_gain = dict()\n",
    "        value_set = list(set(dataset[:,i]))\n",
    "        # 中位点为候选划分点\n",
    "        value_set = middle_point(value_set)\n",
    "        for value in value_set:\n",
    "            matrix_gain[value] = information_gain(dataset, i, value)\n",
    "        info_gain = list(matrix_gain.items())\n",
    "        info_gain.sort(key=lambda x:x[1], reverse=True)\n",
    "        split_val,gain = info_gain[0]\n",
    "        for j in range(len(dataset[:,i])):\n",
    "            if(dataset[j,i] > split_val):\n",
    "                dataset[j,i] = 1\n",
    "            else:\n",
    "                dataset[j,i] = 0\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载数据，随机划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, train_percentage):\n",
    "    \"\"\"\n",
    "    加载数据集，划分训练集和测试集\n",
    "    ：param path:文件路径\n",
    "    ：param train_percentage:训练集所占的比例，默认剩余数据为测试集\n",
    "    :return:划分好的训练集和验证集\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    i = 0\n",
    "    with open(path,'r') as file:\n",
    "        dat = reader(file, delimiter=',')\n",
    "        for row in dat:\n",
    "            # 读取dat文件，并忽略前8行注释\n",
    "            if(i>7):\n",
    "                # 将字符串类型转化为浮点型\n",
    "                row[0:4] = list(map(float, row[0:4]))\n",
    "                dataset.append(row)\n",
    "            i += 1\n",
    "    # 将类别标签转化为整形\n",
    "    for row in dataset:\n",
    "        row[3] = int(row[3])\n",
    "    dataset = np.array(dataset)\n",
    "    dataset = split_attribute(dataset) \n",
    "    # 打乱数据集 \n",
    "    random.shuffle(dataset)\n",
    "    # 划分训练集和验证集\n",
    "    n_train_data = round(len(dataset)*train_percentage)\n",
    "    train_data = dataset[0:n_train_data]\n",
    "    val_data = dataset[n_train_data:]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、 朴素贝叶斯模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 先验概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **若有充足的独立同分布样本，则可以容易地估计出类先验分布概率**\n",
    "$D_c$表示训练集D中第c类样本组成的集合，\n",
    "$$\n",
    "P(c) = \\frac {|D_c|}{|D|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prior(train):\n",
    "    \"\"\"\n",
    "    计算类别的先验概率\n",
    "    :param:训练集\n",
    "    :return:正例、负例的先验概率\n",
    "    \"\"\"\n",
    "    Posi_num = 0\n",
    "    neg_num = 0\n",
    "    for row in train:\n",
    "        if(row[-1] == 1):\n",
    "            Posi_num += 1\n",
    "        else:\n",
    "            neg_num += 1\n",
    "    return [Posi_num/len(train), neg_num/len(train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 后验概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$D_{c,x_i}$表示$D_c$在第i个属性熵取值为$x_i$的样本组成的集合，则条件概率$P(x_i | c)$可估计为\n",
    "\n",
    "$$\n",
    "P(x_i | c) = \\frac{|D_{c,x_i}|}{|D_c|}\n",
    "$$\n",
    "\n",
    "基于属性条件独立性假设，后验概率可重写为\n",
    "$$\n",
    "P(c|x) = \\frac {P(c)P(x|c)}{P(x)} = \\frac {P(c)}{P(x)}\\prod_{i=1}^d P(x_i | c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **考虑是否需要在估计概率值时进行平滑(smoothing)**\n",
    "##### 拉普拉斯修正（Laplacian correction),令N表示训练集D中可能的类别数，$N_i$表示第i个属性可能的取值数，则先验概率修正为：\n",
    "$$\n",
    "\\hat P(c) = \\frac {|D_c|+1}{|D+N|}\n",
    "$$\n",
    "条件概率修正为：\n",
    "\n",
    "$$\n",
    "\\hat P(x_i | c) = \\frac {|D_{c,x_i}|+1}{|D_c|+N_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### **实验发现此数据集的属性值分布较为均匀，平滑系数的引入与否对提高正确率没有影响，故不引入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_pr(dataset, label, attribute_index, value):\n",
    "    \"\"\"\n",
    "    给定label下，计算观测到所给特征值的条件概率\n",
    "    :param: dataset：训练集\n",
    "    :label:所属类别\n",
    "    :attribute_index:观测值所在特征列的列号\n",
    "    :value：观测值\n",
    "    :return：条件概率\n",
    "    \"\"\"\n",
    "    this_dataset = []\n",
    "    this_dataset_value = []\n",
    "    for row in dataset:\n",
    "        if(row[-1] == label):\n",
    "            this_dataset.append(row)\n",
    "    for row in this_dataset:\n",
    "        if(row[attribute_index] == value):\n",
    "            this_dataset_value.append(row)\n",
    "    return (len(this_dataset_value) / (len(this_dataset)))\n",
    "\n",
    "\n",
    "def Posteir(data, label):\n",
    "    \"\"\"\n",
    "    根据观测值，计算样本属于某一类别的后验概率\n",
    "    :param:data：样本数据\n",
    "    :label:假设样本属于的某一类别\n",
    "    :return:后验概率\n",
    "    \"\"\"\n",
    "    Pr_P,Pr_N = Prior(train)\n",
    "    # 当前类别下的概率\n",
    "    Pr_x_c = 1\n",
    "    # 另外一类的概率\n",
    "    Pr_x_d = 1\n",
    "    for i in range(len(data)):\n",
    "        Pr_x_c = Pr_x_c*conditional_pr(train, label, i, data[i])\n",
    "    for i in range(len(data)):\n",
    "        Pr_x_d = Pr_x_d*conditional_pr(train, -label, i, data[i])    \n",
    "    if(label == 1):\n",
    "        Pr_x_c = Pr_x_c * Pr_P\n",
    "        Pr_x_d = Pr_x_d * Pr_N\n",
    "    else:\n",
    "        Pr_x_c = Pr_x_c * Pr_N\n",
    "        Pr_x_d = Pr_x_d * Pr_P\n",
    "    return (Pr_x_c/(Pr_x_c+Pr_x_d)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_classifier(data):\n",
    "    \"\"\"\n",
    "    根据训练得到的贝叶斯分类器，对所给样本所属类别进行预测\n",
    "    :param:data:样本\n",
    "    :return: 预测类标，预测类标为1的后验概率，预测类别为-1的后验概率\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for label in [-1,1]:\n",
    "        result.append(Posteir(data,label))\n",
    "    if(result.index(max(result)) == 0):\n",
    "        return -1,Posteir(data,1),Posteir(data,-1)\n",
    "    else:\n",
    "        return 1,Posteir(data,1),Posteir(data,-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(val_data):\n",
    "    \"\"\"测试模型在验证集上的效果\n",
    "    :param val_data: 验证集\n",
    "    :return: 模型在验证集上的准确率\n",
    "    \"\"\"\n",
    "    # 获取预测类标\n",
    "    predicted_label = []\n",
    "    Pr_N = []\n",
    "    Pr_P = []\n",
    "    for row in val_data:\n",
    "        result = Bayes_classifier(row[0:3])\n",
    "        prediction = result[0]\n",
    "        Pr_P.append(result[1])\n",
    "        Pr_N.append(result[2])\n",
    "        predicted_label.append(prediction)\n",
    "    # 获取真实类标\n",
    "    actual_label = [row[-1] for row in val_data]\n",
    "    # 计算准确率\n",
    "    accuracy = accuracy_calculation(actual_label, predicted_label)\n",
    "    return round(accuracy,2),predicted_label,Pr_P,Pr_N\n",
    "\n",
    "def accuracy_calculation(actual_label, predicted_label):\n",
    "    \"\"\"计算准确率\n",
    "    :param actual_label: 真实类标\n",
    "    :param predicted_label: 模型预测的类标\n",
    "    :return: 准确率（百分制）\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "    for i in range(len(actual_label)):\n",
    "        if actual_label[i] == predicted_label[i]:\n",
    "            correct_count += 1\n",
    "    return correct_count / float(len(actual_label)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5452d1222b79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# 训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_percentage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPr_P\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPr_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"accuracy is: {result}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#储存结果: 将每个样例所属每个类别的后验概率值输入到新的文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8d4598ef8c06>\u001b[0m in \u001b[0;36mvalidation\u001b[1;34m(val_data)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mPr_P\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBayes_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mPr_P\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-0f49f6317a26>\u001b[0m in \u001b[0;36mBayes_classifier\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPosteir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPosteir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPosteir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-853871c9c3ac>\u001b[0m in \u001b[0;36mPosteir\u001b[1;34m(data, label)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mPr_x_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPr_x_c\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconditional_pr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mPr_x_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPr_x_d\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconditional_pr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mPr_x_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPr_x_c\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mPr_P\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-853871c9c3ac>\u001b[0m in \u001b[0;36mconditional_pr\u001b[1;34m(dataset, label, attribute_index, value)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mthis_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = './titanic.dat'\n",
    "\n",
    "    # 参数设置\n",
    "    train_percentage = 0.3\n",
    "    file_save_name = 'Bayes_Titanic_result.txt' # 结果保存路径：默认当前文件路径下\n",
    "    # 训练模型\n",
    "    train, val = load_dataset(file_path, train_percentage)\n",
    "    result, predicted_label,Pr_P,Pr_N = validation(val)\n",
    "    print(f\"accuracy is: {result}%\")\n",
    "    #储存结果: 将每个样例所属每个类别的后验概率值输入到新的文件\n",
    "    val = pd.DataFrame(val)\n",
    "    val['predicted_survived'] = predicted_label\n",
    "    val['Predicted_label_1'] = Pr_P\n",
    "    val['Predicted_label_-1'] = Pr_N\n",
    "    val.rename(columns={0:'Class', 1:'Age', 2:'Sex',3:'Survived'}, inplace = True)\n",
    "    val.to_csv(file_save_name,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 78.281%\n"
     ]
    }
   ],
   "source": [
    "result = np.array([])\n",
    "for i in range(10):\n",
    "    train, val = load_dataset(file_path, train_percentage)\n",
    "    result = np.append(result,validation(val)[0])\n",
    "print(f\"accuracy is: {np.mean(result)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.46"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.56"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.96"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array([1,2,3])\n",
    "np.append(s,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 番外:**引入拉普拉斯修正**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **考虑是否需要在估计概率值时进行平滑(smoothing)**\n",
    "##### 拉普拉斯修正（Laplacian correction),令N表示训练集D中可能的类别数，$N_i$表示第i个属性可能的取值数，则先验概率修正为：\n",
    "$$\n",
    "\\hat P(c) = \\frac {|D_c|+1}{|D+N|}\n",
    "$$\n",
    "条件概率修正为：\n",
    "\n",
    "$$\n",
    "\\hat P(x_i | c) = \\frac {|D_{c,x_i}|+1}{|D_c|+N_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入平滑系数\n",
    "def Prior(train):\n",
    "    \"\"\"\n",
    "    计算类别的先验概率\n",
    "    :param:训练集\n",
    "    :return:正例、负例的先验概率\n",
    "    \"\"\"\n",
    "    Posi_num = 0\n",
    "    neg_num = 0\n",
    "    for row in train:\n",
    "        if(row[-1] == 1):\n",
    "            Posi_num += 1\n",
    "        else:\n",
    "            neg_num += 1\n",
    "    return [(1+Posi_num)/(len(train)+2), (1+neg_num)/(2+len(train))]\n",
    "def conditional_pr(dataset, label, attribute_index, value):\n",
    "    \"\"\"\n",
    "    给定label下，计算观测到所给特征值的条件概率\n",
    "    :param: dataset：训练集\n",
    "    :label:所属类别\n",
    "    :attribute_index:观测值所在特征列的列号\n",
    "    :value：观测值\n",
    "    :return：条件概率\n",
    "    \"\"\"\n",
    "    this_dataset = []\n",
    "    this_dataset_value = []\n",
    "    \n",
    "    for row in dataset:\n",
    "        if(row[-1] == label):\n",
    "            this_dataset.append(row)\n",
    "    for row in this_dataset:\n",
    "        if(row[attribute_index] == value):\n",
    "            this_dataset_value.append(row)\n",
    "    return ((1+len(this_dataset_value)) / (len(this_dataset)+len(set(train[:,attribute_index]))))\n",
    "\n",
    "\n",
    "def Posteir(data, label):\n",
    "    \"\"\"\n",
    "    根据观测值，计算样本属于某一类别的后验概率\n",
    "    :param:data：样本数据\n",
    "    :label:假设样本属于的某一类别\n",
    "    :return:后验概率\n",
    "    \"\"\"\n",
    "    Pr_P,Pr_N = Prior(train)\n",
    "    # 当前类别下的概率\n",
    "    Pr_x_c = 1\n",
    "    # 另外一类的概率\n",
    "    Pr_x_d = 1\n",
    "    for i in range(len(data)):\n",
    "        Pr_x_c = Pr_x_c*conditional_pr(train, label, i, data[i])\n",
    "    for i in range(len(data)):\n",
    "        Pr_x_d = Pr_x_d*conditional_pr(train, -label, i, data[i])    \n",
    "    if(label == 1):\n",
    "        Pr_x_c = Pr_x_c * Pr_P\n",
    "        Pr_x_d = Pr_x_d * Pr_N\n",
    "    else:\n",
    "        Pr_x_c = Pr_x_c * Pr_N\n",
    "        Pr_x_d = Pr_x_d * Pr_P\n",
    "    return (Pr_x_c/(Pr_x_c+Pr_x_d)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. **机器学习，周志华，清华大学出版社**\n",
    "2. **Data Mining: Concepts and Techniques, Jiawei Han, Micheline kamber, Jian pei**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def func():\n",
    "    return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  信息熵是度量样本集合纯度最常用的一种指标，假定当前样本集合D中第k类样本所占比例为$p_k (k = 1,2,...|\\Upsilon|)$,则D的信息熵定义为\n",
    "\n",
    "$$\n",
    "Ent(D) = -\\sum_{k=1}^{|\\Upsilon|} p_klog_2p_k .\n",
    "$$ \n",
    "> #### 假定离散属性值a有V个可能的取值${a^1,a^2,...,a^V}$,若使用a对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为$a^v$的样本，记位$D^v$. 可根据上式计算$D_v$的信息熵，考虑道不同的分支节点所包含的样本数不同，给分支结点赋予权重$\\frac {|D^v|}{|D|}$,即样本数越多的分支结点影响越大，于是可计算出用属性a对样本集D进行划分所获得的信息增益：\n",
    "\n",
    "$$\n",
    "Gain(D, a) = Ent(D)-\\sum_{v=1}^V\\frac{|D^v|}{|D|}Ent(D^v)\n",
    "$$\n",
    "> #### 一般而言，信息增益越大，则意味者使用属性a来进化划分所获得的\"纯度提升\"越大，即选择属性$a_* = arg\\,max\\,Gain(D,a)\\,,a\\in A$来进行划分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
